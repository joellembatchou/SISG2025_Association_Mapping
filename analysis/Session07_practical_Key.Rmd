---
title: "Practical 7 Key - Rare Variant Analysis"
author: ""
date: ""
output: html_document
editor_options:
  chunk_output_type: console
---
Before you begin:

* Make sure that R is installed on your computer
* For this lab, we will use the following R libraries:
```{r load-libs, message=FALSE, warning=FALSE}
library(data.table)
library(dplyr)
library(BEDMatrix)
library(SKAT)
library(ACAT)
library(ggplot2)
```

## Introduction
We will look into a dataset collected on a quantitative phenotype which was first analyzed through GWAS and a signal was detected in a region on chromosome 1. Let's determine whether the signal is present when we focus on rare variation at the locus. In our analyses, ***we will define rare variants as those with $MAF \leq 5\%$***.

The file ["rv_pheno.txt"](https://github.com/joellembatchou/SISG2024_Association_Mapping/tree/master/data)” contains the phenotype measurements for a set of individuals and the file ["rv_geno_chr1.bed"](https://github.com/joellembatchou/SISG2024_Association_Mapping/tree/master/data) is a binary file in PLINK BED format with accompanying BIM and FAM files which contains the genotype data.

### Data preparation
Let’s first load the files into the R session. We first need to define the path to the directory containing the phenotype and genotype files **(change the path to the files location on your machine)**.
```{r echo=FALSE}
files_dir <- "/Users/joelle.mbatchou/SISG/2024/SISG2024_Association_Mapping/data/" 
plink2_binary <- "/Users/joelle.mbatchou/software/bins/plink2" 
```

```{r eval=FALSE}
files_dir <- "/SISGM19/data/" 
```

Also specify the paths to the PLINK2 binary:
```{r eval=FALSE}
plink2_binary <- "/SISGM19/bin/plink2" 
```

We can now read the phenotype file:
```{r}
pheno_file <- fread(sprintf("%s/rv_pheno.txt", files_dir), header = TRUE) 
head(pheno_file, 3)
```

### Exercises
## Exercises
1.  Using PLINK, extract **rare variants** in a new PLINK BED file.
We use options `--max-maf` to select rare variants and `--maj-ref force` so that the minor allele is the effect allele.
```{r}
# first fill in the thresholds to use for each filter
filter_maf = 0.05

cmd <- sprintf('%s --bfile "%s/rv_geno_chr1" --max-maf %g --maj-ref force --make-bed --out chr1_region_rv', plink2_binary, files_dir, filter_maf)
system(cmd)
```
This generates a new set of PLINK BED genotype files containing only the variants that passed the MAF filter `chr1_region_rv.{bed,bim,fam}`.

2. Import the data files in R.

* Read the genotype data using R function `BEDMatrix()`. We use option `simple_names = TRUE` to easily filter by sample IDs later.
```{r}
G <- BEDMatrix("chr1_region_rv", simple_names = TRUE)
```

* Keep only samples who are present both in the genotype as well as phenotype data and who don't have missing values for the phenotype.
```{r}
# identify ID of samples with non-missing phenotypes
ids.keep <- with(pheno_file, IID[ !is.na(Pheno) ])
# subset the genotype & phenotype data based on IDs
G <- G[match(ids.keep, rownames(G)), ]
pheno_file <- pheno_file[match(ids.keep, pheno_file$IID), ] 
```

3. Examine the genotype data:
  * Compute the minor allele frequency (MAF) for each SNP and plot histogram. We use the `colMeans()` function to G and specify the argument na.rm = TRUE in case missing genotypes are present.
```{r}
# Recall the MAF formula: maf(g) = sum(g) / (2*N) = mean(g) / 2
maf <- colMeans(G, na.rm=TRUE)/2
# we can use the 'hist' function in R to plot histograms
hist(maf, xlab = "Minor allele frequencies", main = "Distribution of MAF")
```


4. Run the single variant association tests in PLINK (only for the extracted variants).
```{r}
cmd <- sprintf("%s --bfile chr1_region_rv --pheno %s/rv_pheno.txt --pheno-name Pheno --glm allow-no-covars  --out test_plink", plink2_binary, files_dir)
system(cmd)
sv_pvals <- fread("test_plink.Pheno.glm.linear")
str(sv_pvals) # what variables are used to store p-values and effect sizes?
```

  * What would be your significance threshold after applying Bonferroni correction for the multiple tests (assume the nominal significance level is 0.05)? Is anything significant after this correction?
```{r}
# determine the appropriate significance threshold
bonf_threshold <- 0.05 / length(sv_pvals$P) 
bonf_threshold
sv_pvals[ sv_pvals$P < bonf_threshold, ]
```
  
  * Make a volcano plot (i.e. plot log10 p-values against the effect sizes). Which of the Burden/SKAT/ACAT tests do you expect will give us most power? 

```{r}
with(sv_pvals, plot(x=BETA, y=-log10(sv_pvals$P), xlab = "Effect size", ylab = "-log10P")) 
```

5. We will first compare three collapsing/burden approaches:
  * CAST (Binary collapsing approach): for each individual, count if they have a rare allele at any of the sites
  * MZ Test/GRANVIL (Count based collapsing): for each individual, count the total number of sites where a rare allele is present
  * Weighted burden test: for each individual, take a weighted count of the rare alleles across sites (for the weights, use `weights <- dbeta(MAF, 1, 25)`)

For each approach, first need to generate the burden scores. 
```{r}
# CAST : count number of rare alleles for each person and determine if it is > 0
sum_alleles_per_sample <- rowSums(G)
burden.cast <- as.numeric( sum_alleles_per_sample > 0 )

# MZ : count number of sites with rare alleles for each person
burden.mz <- rowSums(G > 0)

# Weighted burden : weighted sum of genotype counts across sites
weights <- dbeta(maf, 1, 25) 
burden.weighted <- G %*% weights
```

Run a test for association between the phenotype and each burden score using the `lm()` R function, e.g.
```{r}
# e.g. for CAST
summary(lm(pheno_file$Pheno ~ burden.cast))
# for MZ
summary(lm(pheno_file$Pheno ~ burden.mz))
# Weighted burden
summary(lm(pheno_file$Pheno ~ burden.weighted))
```
Looking further at the weighted burden variant weights
```{r}
# across a wide MAF range
maf_ranges <- seq(.01,.5, length=100)
plot(dbeta(maf_ranges, 1, 25)  ~ maf_ranges, type = "l", xlab = "MAF", ylab = "Variant weights in weighted burden test")
points(weights ~ maf, col = "red", pch = 18)

# only looking at variants in the data
plot(weights ~ maf, col = "red", pch = 18, xlab = "MAF", ylab = "Variant weights in weighted burden test")

```


6. Now use SKAT to test for an association. 
```{r}
# first fit the null model
skat.null <- SKAT_Null_Model( pheno_file$Pheno ~ 1 , out_type = "C")
# Run SKAT association test (returns a list - p-value is in `$p.value`)
skat_sumstats <- SKAT(G, skat.null )
str(skat_sumstats)
skat_sumstats$p.value
```

7. Run the omnibus SKAT, but consider setting $\rho$ (i.e.`r.corr`) to 0 and then 1. 
  * Compare the results to using the CAST,MZ/GRANVIL and Weighted burden collapsing approaches in Question 5 as well as SKAT in Question 6. What tests do these $\rho$ values correspond to?
```{r}
# Example for rho = 0
rho <- 0
skat_sumstats_rho <- SKAT(G, skat.null, r.corr = rho )
skat_sumstats_rho$p.value # = SKAT test
rho <- 1
skat_sumstats_rho <- SKAT(G, skat.null, r.corr = rho )
skat_sumstats_rho$p.value # = weighted burden test
```

8. Now consider the omnibus version of SKAT, but use the “optimal.adj” approach which searches across a range of rho values.
```{r}
# Run SKATO association test using grid of rho values
skat_sumstats_rho_grid <- SKAT(G, skat.null, method="optimal.adj")
skat_sumstats_rho_grid$p.value
```

9. Run ACATV on the single variant p-values.
The basic command would look like
```{r}
# `weights` vector is from Question 5
acat.weights <- weights * weights * maf * (1 - maf)
ACAT(sv_pvals$P, weights = acat.weights)
```

10. Run ACATO combining the SKAT and BURDEN p-values (from Question 7) with the ACATV p-value (from Question 9).
```{r}
# Fill in the p-values
SKAT_pvalue <- 8.745405e-11
Burden_pvalue <- 0.2234603
ACATV_pvalue <- 0.00112117
# compute ACATO
ACAT( c(SKAT_pvalue, Burden_pvalue, ACATV_pvalue) )
```

### Extra

You can explore the impact of different genetic architectures by analyzing 3 simulated phenotypes in ["data/rv_pheno_extended.txt"](https://gatech.box.com/s/espcwnlibsw62xoiqska9ib9iccxs5fk). More specifically, you can run the same analyses done above for each of these traits and compare the performance of the tests relative to how variant effects were simulated:

* "Pheno_sparse": a single causal variant (i.e. sparse genetic architecture)
* "Pheno_burden_skat": 60% of the variant are causal, and they all have positive effects on the trait
* "Pheno_skat": 20% of the variants are causal and the effect direction (+/-) is randomly assigned

Here is R code used to generate the phenotypes:
```{r eval=FALSE}
G <- BEDMatrix::BEDMatrix("data/rv_geno_chr1", simple_names = TRUE)
N <- nrow(G)
nsnps <- ncol(G)

# Parameters to change
# sparse : set n.causal = 1
# burden_skat : set prop_causal=0.6 and prop_pos_beta=1
# skat : set prop_causal=0.2 and prop_pos_beta=0.5
prop_causal <- 1
prop_pos_beta <- 1
h2g <- 0.01

n.causal <- round(nsnps * prop_causal)
causal.index <- sample(ncol(G), n.causal)

b_snp <- sqrt(h2g/n.causal)
beta <- rep(b_snp, n.causal)
beta_sign <- sample(c(rep(1, round(n.causal * prop_pos_beta)), rep(-1, n.causal - round(n.causal * prop_pos_beta))))
h2e <- 1 - h2g

y <- scale(G[, causal.index]) %*% beta + rnorm(N, sd = sqrt(h2e))
```


```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# cleanup
file.remove(list.files(pattern = "chr1_region_rv*"))
file.remove(list.files(pattern = "sv_test*"))

```
